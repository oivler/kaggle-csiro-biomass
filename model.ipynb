{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6352a5f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "DATA_DIR = '/kaggle/input/csiro-biomass'\n",
    "WORKING_DIR = '/kaggle/working'\n",
    "\n",
    "USE_PRETRAINED_WEIGHTS = True\n",
    "PRETRAINED_WEIGHTS_DIR = '/kaggle/input/pretrained-weights/pretrained_weights'\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Using pretrained weights: {USE_PRETRAINED_WEIGHTS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(\"\\nFirst few rows of train:\")\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf096e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "target_names = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "\n",
    "unique_image_paths = train_df['image_path'].unique()\n",
    "print(f\"Total rows in train.csv: {len(train_df)}\")\n",
    "print(f\"Number of unique image paths: {len(unique_image_paths)}\")\n",
    "print(f\"Expected: {len(train_df)} / 5 targets = {len(train_df) // 5} unique images\")\n",
    "\n",
    "train_images = []\n",
    "train_targets = []\n",
    "\n",
    "for img_path in unique_image_paths:\n",
    "    targets = train_df[train_df['image_path'] == img_path].set_index('target_name')['target'].to_dict()\n",
    "    full_img_path = os.path.join(DATA_DIR, img_path)\n",
    "    train_images.append(full_img_path)\n",
    "    train_targets.append([targets[col] for col in target_cols])\n",
    "\n",
    "train_data = pd.DataFrame({\n",
    "    'image_path': train_images,\n",
    "    **{col: [t[i] for t in train_targets] for i, col in enumerate(target_cols)}\n",
    "})\n",
    "\n",
    "print(f\"\\nReshaped training data: {train_data.shape}\")\n",
    "print(f\"This means we have {train_data.shape[0]} unique images\")\n",
    "print(train_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5d9ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.target_cols = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "        self.has_targets = all(col in dataframe.columns for col in self.target_cols)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['image_path']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.has_targets:\n",
    "            targets = torch.FloatTensor([\n",
    "                self.dataframe.iloc[idx][col] for col in self.target_cols\n",
    "            ])\n",
    "        else:\n",
    "            targets = torch.FloatTensor([0.0] * len(self.target_cols))\n",
    "\n",
    "        return image, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dacb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class AugmentedBiomassDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, augment=True):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.num_augmentations = 6\n",
    "        self.target_cols = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "        self.has_targets = all(col in dataframe.columns for col in self.target_cols)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.augment:\n",
    "            return len(self.dataframe) * self.num_augmentations\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.augment:\n",
    "            actual_idx = idx // self.num_augmentations\n",
    "            aug_type = idx % self.num_augmentations\n",
    "        else:\n",
    "            actual_idx = idx\n",
    "            aug_type = 0\n",
    "\n",
    "        img_path = self.dataframe.iloc[actual_idx]['image_path']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.augment and aug_type > 0:\n",
    "            if aug_type == 1:\n",
    "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            elif aug_type == 2:\n",
    "                image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            elif aug_type == 3:\n",
    "                image = image.rotate(90, expand=True)\n",
    "            elif aug_type == 4:\n",
    "                image = image.rotate(180, expand=True)\n",
    "            elif aug_type == 5:\n",
    "                image = image.rotate(270, expand=True)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.has_targets:\n",
    "            targets = torch.FloatTensor([\n",
    "                self.dataframe.iloc[actual_idx][col] for col in self.target_cols\n",
    "            ])\n",
    "        else:\n",
    "            targets = torch.FloatTensor([0.0] * len(self.target_cols))\n",
    "\n",
    "        return image, targets\n",
    "\n",
    "print(f\"Full training data: {len(train_data)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 2 if torch.cuda.is_available() else 0\n",
    "print(f\"Batch size: {batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_weights(model, weights_path):\n",
    "    if os.path.exists(weights_path):\n",
    "        try:\n",
    "            state_dict = torch.load(weights_path, map_location='cpu')\n",
    "            if 'state_dict' in state_dict:\n",
    "                state_dict = state_dict['state_dict']\n",
    "            if any(k.startswith('module.') for k in state_dict.keys()):\n",
    "                state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "            filtered_dict = {}\n",
    "            for k, v in state_dict.items():\n",
    "                if not (k.startswith('fc.') or k.startswith('classifier.')):\n",
    "                    filtered_dict[k] = v\n",
    "\n",
    "            model.load_state_dict(filtered_dict, strict=False)\n",
    "            print(f\"Loaded pretrained weights from {os.path.basename(weights_path)}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading weights from {weights_path}: {e}\")\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "class BiomassPredictorResNet18(nn.Module):\n",
    "    def __init__(self, num_targets=5, pretrained_weights_path=None):\n",
    "        super(BiomassPredictorResNet18, self).__init__()\n",
    "        self.backbone = models.resnet18(weights=None)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        if pretrained_weights_path:\n",
    "            load_pretrained_weights(self.backbone, pretrained_weights_path)\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_targets)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "class BiomassPredictorResNet34(nn.Module):\n",
    "    def __init__(self, num_targets=5, pretrained_weights_path=None):\n",
    "        super(BiomassPredictorResNet34, self).__init__()\n",
    "        self.backbone = models.resnet34(weights=None)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        if pretrained_weights_path:\n",
    "            load_pretrained_weights(self.backbone, pretrained_weights_path)\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_targets)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "class BiomassPredictorResNet50(nn.Module):\n",
    "    def __init__(self, num_targets=5, pretrained_weights_path=None):\n",
    "        super(BiomassPredictorResNet50, self).__init__()\n",
    "        self.backbone = models.resnet50(weights=None)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        if pretrained_weights_path:\n",
    "            load_pretrained_weights(self.backbone, pretrained_weights_path)\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_targets)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "class BiomassPredictorEfficientNet(nn.Module):\n",
    "    def __init__(self, num_targets=5, pretrained_weights_path=None):\n",
    "        super(BiomassPredictorEfficientNet, self).__init__()\n",
    "        self.backbone = models.efficientnet_b0(weights=None)\n",
    "        num_features = self.backbone.classifier[1].in_features\n",
    "        if pretrained_weights_path:\n",
    "            load_pretrained_weights(self.backbone, pretrained_weights_path)\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_targets)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "print(\"Model architectures defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a4160",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "pretrained_paths = {}\n",
    "if USE_PRETRAINED_WEIGHTS and os.path.exists(PRETRAINED_WEIGHTS_DIR):\n",
    "    print(f\"Found pretrained weights directory: {PRETRAINED_WEIGHTS_DIR}\")\n",
    "    weight_files = {\n",
    "        'ResNet18': 'resnet18-f37072fd.pth',\n",
    "        'ResNet34': 'resnet34-b627a593.pth',\n",
    "        'ResNet50': 'resnet50-0676ba61.pth',\n",
    "        'EfficientNet_B0': 'efficientnet_b0_rwightman-3dd342df.pth',\n",
    "    }\n",
    "    for model_name, filename in weight_files.items():\n",
    "        weight_path = os.path.join(PRETRAINED_WEIGHTS_DIR, filename)\n",
    "        if os.path.exists(weight_path):\n",
    "            pretrained_paths[model_name] = weight_path\n",
    "            print(f\"  Found {model_name} weights: {filename}\")\n",
    "        else:\n",
    "            print(f\"  Warning: {model_name} weights not found\")\n",
    "else:\n",
    "    print(f\"Pretrained weights directory not found: {PRETRAINED_WEIGHTS_DIR}\")\n",
    "    USE_PRETRAINED_WEIGHTS = False\n",
    "\n",
    "models_to_train = {}\n",
    "for model_name in ['ResNet18', 'ResNet34', 'ResNet50', 'EfficientNet_B0']:\n",
    "    weight_path = pretrained_paths.get(model_name) if USE_PRETRAINED_WEIGHTS else None\n",
    "\n",
    "    if model_name == 'ResNet18':\n",
    "        models_to_train[model_name] = BiomassPredictorResNet18(num_targets=5, pretrained_weights_path=weight_path)\n",
    "    elif model_name == 'ResNet34':\n",
    "        models_to_train[model_name] = BiomassPredictorResNet34(num_targets=5, pretrained_weights_path=weight_path)\n",
    "    elif model_name == 'ResNet50':\n",
    "        models_to_train[model_name] = BiomassPredictorResNet50(num_targets=5, pretrained_weights_path=weight_path)\n",
    "    elif model_name == 'EfficientNet_B0':\n",
    "        models_to_train[model_name] = BiomassPredictorEfficientNet(num_targets=5, pretrained_weights_path=weight_path)\n",
    "\n",
    "print(f\"Initialized {len(models_to_train)} models: {list(models_to_train.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "    return running_loss / len(val_loader)\n",
    "\n",
    "\n",
    "def train_model(model, model_name, train_loader, val_loader, num_epochs=30, lr=0.001, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "\n",
    "        print(f\"{model_name} Epoch {epoch+1}/{num_epochs} - Train {train_loss:.4f} Val {val_loss:.4f} LR {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    return best_model_state, best_val_loss\n",
    "\n",
    "print('Training functions defined!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ef2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "model_states = {}\n",
    "val_losses = {}\n",
    "num_epochs = 30\n",
    "initial_lr = 0.001\n",
    "\n",
    "full_train_data = train_data.copy()\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(full_train_data)):\n",
    "    fold_train_data = full_train_data.iloc[train_idx].reset_index(drop=True)\n",
    "    fold_val_data = full_train_data.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    fold_train_dataset = AugmentedBiomassDataset(fold_train_data, transform=transform_train, augment=True)\n",
    "    fold_val_dataset = BiomassDataset(fold_val_data, transform=transform_val)\n",
    "\n",
    "    fold_train_loader = DataLoader(\n",
    "        fold_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    fold_val_loader = DataLoader(\n",
    "        fold_val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Fold {fold_idx + 1} - Train: {len(fold_train_dataset)}, Val: {len(fold_val_dataset)}\")\n",
    "\n",
    "    for model_name in models_to_train.keys():\n",
    "        weight_path = pretrained_paths.get(model_name) if USE_PRETRAINED_WEIGHTS else None\n",
    "\n",
    "        if model_name == 'ResNet18':\n",
    "            model = BiomassPredictorResNet18(num_targets=5, pretrained_weights_path=weight_path)\n",
    "        elif model_name == 'ResNet34':\n",
    "            model = BiomassPredictorResNet34(num_targets=5, pretrained_weights_path=weight_path)\n",
    "        elif model_name == 'ResNet50':\n",
    "            model = BiomassPredictorResNet50(num_targets=5, pretrained_weights_path=weight_path)\n",
    "        elif model_name == 'EfficientNet_B0':\n",
    "            model = BiomassPredictorEfficientNet(num_targets=5, pretrained_weights_path=weight_path)\n",
    "\n",
    "        full_model_name = f\"{model_name}_fold{fold_idx + 1}\"\n",
    "        model_state, val_loss = train_model(\n",
    "            model,\n",
    "            full_model_name,\n",
    "            fold_train_loader,\n",
    "            fold_val_loader,\n",
    "            num_epochs=num_epochs,\n",
    "            lr=initial_lr,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        model_states[full_model_name] = model_state\n",
    "        val_losses[full_model_name] = val_loss\n",
    "        torch.save(model_state, f'{WORKING_DIR}/best_model_{full_model_name}.pth')\n",
    "\n",
    "print(f\"All K-Fold models trained! Total models: {len(model_states)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b6efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self, models_dict, model_states, val_losses, device):\n",
    "        self.models = {}\n",
    "        self.device = device\n",
    "        if val_losses:\n",
    "            inv_losses = {name: 1.0 / (loss + 1e-8) for name, loss in val_losses.items()}\n",
    "            total_inv = sum(inv_losses.values())\n",
    "            self.weights = {name: inv_loss / total_inv for name, inv_loss in inv_losses.items()}\n",
    "        else:\n",
    "            num_models = len(model_states)\n",
    "            self.weights = {name: 1.0 / num_models for name in model_states.keys()}\n",
    "\n",
    "        for name, model_state in model_states.items():\n",
    "            base_name = name.split('_fold')[0]\n",
    "            if base_name in models_dict:\n",
    "                weight_path = pretrained_paths.get(base_name) if USE_PRETRAINED_WEIGHTS else None\n",
    "                if base_name == 'ResNet18':\n",
    "                    model = BiomassPredictorResNet18(num_targets=5, pretrained_weights_path=weight_path).to(device)\n",
    "                elif base_name == 'ResNet34':\n",
    "                    model = BiomassPredictorResNet34(num_targets=5, pretrained_weights_path=weight_path).to(device)\n",
    "                elif base_name == 'ResNet50':\n",
    "                    model = BiomassPredictorResNet50(num_targets=5, pretrained_weights_path=weight_path).to(device)\n",
    "                elif base_name == 'EfficientNet_B0':\n",
    "                    model = BiomassPredictorEfficientNet(num_targets=5, pretrained_weights_path=weight_path).to(device)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                model.load_state_dict(model_state)\n",
    "                model.eval()\n",
    "                self.models[name] = model\n",
    "\n",
    "    def predict(self, dataloader, use_tta=True):\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for images, _ in dataloader:\n",
    "                images = images.to(self.device, non_blocking=True)\n",
    "\n",
    "                if use_tta:\n",
    "                    tta_preds = []\n",
    "\n",
    "                    model_preds_weighted = []\n",
    "                    for name, model in self.models.items():\n",
    "                        pred = model(images).cpu().numpy()\n",
    "                        weight = self.weights.get(name, 1.0 / len(self.models))\n",
    "                        model_preds_weighted.append(pred * weight)\n",
    "                    tta_preds.append(np.sum(model_preds_weighted, axis=0))\n",
    "\n",
    "                    images_hflip = torch.flip(images, [3])\n",
    "                    model_preds_weighted = []\n",
    "                    for name, model in self.models.items():\n",
    "                        pred = model(images_hflip).cpu().numpy()\n",
    "                        weight = self.weights.get(name, 1.0 / len(self.models))\n",
    "                        model_preds_weighted.append(pred * weight)\n",
    "                    tta_preds.append(np.sum(model_preds_weighted, axis=0))\n",
    "\n",
    "                    images_vflip = torch.flip(images, [2])\n",
    "                    model_preds_weighted = []\n",
    "                    for name, model in self.models.items():\n",
    "                        pred = model(images_vflip).cpu().numpy()\n",
    "                        weight = self.weights.get(name, 1.0 / len(self.models))\n",
    "                        model_preds_weighted.append(pred * weight)\n",
    "                    tta_preds.append(np.sum(model_preds_weighted, axis=0))\n",
    "\n",
    "                    images_90 = torch.rot90(images, k=1, dims=[2, 3])\n",
    "                    model_preds_weighted = []\n",
    "                    for name, model in self.models.items():\n",
    "                        pred = model(images_90).cpu().numpy()\n",
    "                        weight = self.weights.get(name, 1.0 / len(self.models))\n",
    "                        model_preds_weighted.append(pred * weight)\n",
    "                    tta_preds.append(np.sum(model_preds_weighted, axis=0))\n",
    "\n",
    "                    images_180 = torch.rot90(images, k=2, dims=[2, 3])\n",
    "                    model_preds_weighted = []\n",
    "                    for name, model in self.models.items():\n",
    "                        pred = model(images_180).cpu().numpy()\n",
    "                        weight = self.weights.get(name, 1.0 / len(self.models))\n",
    "                        model_preds_weighted.append(pred * weight)\n",
    "                    tta_preds.append(np.sum(model_preds_weighted, axis=0))\n",
    "\n",
    "                    images_270 = torch.rot90(images, k=3, dims=[2, 3])\n",
    "                    model_preds_weighted = []\n",
    "                    for name, model in self.models.items():\n",
    "                        pred = model(images_270).cpu().numpy()\n",
    "                        weight = self.weights.get(name, 1.0 / len(self.models))\n",
    "                        model_preds_weighted.append(pred * weight)\n",
    "                    tta_preds.append(np.sum(model_preds_weighted, axis=0))\n",
    "\n",
    "                    images_hflip_90 = torch.rot90(images_hflip, k=1, dims=[2, 3])\n",
    "                    model_preds_weighted = []\n",
    "                    for name, model in self.models.items():\n",
    "                        pred = model(images_hflip_90).cpu().numpy()\n",
    "                        weight = self.weights.get(name, 1.0 / len(self.models))\n",
    "                        model_preds_weighted.append(pred * weight)\n",
    "                    tta_preds.append(np.sum(model_preds_weighted, axis=0))\n",
    "\n",
    "                    images_vflip_90 = torch.rot90(images_vflip, k=1, dims=[2, 3])\n",
    "                    model_preds_weighted = []\n",
    "                    for name, model in self.models.items():\n",
    "                        pred = model(images_vflip_90).cpu().numpy()\n",
    "                        weight = self.weights.get(name, 1.0 / len(self.models))\n",
    "                        model_preds_weighted.append(pred * weight)\n",
    "                    tta_preds.append(np.sum(model_preds_weighted, axis=0))\n",
    "\n",
    "                    ensemble_pred = np.mean(tta_preds, axis=0)\n",
    "                else:\n",
    "                    model_preds_weighted = []\n",
    "                    for name, model in self.models.items():\n",
    "                        pred = model(images).cpu().numpy()\n",
    "                        weight = self.weights.get(name, 1.0 / len(self.models))\n",
    "                        model_preds_weighted.append(pred * weight)\n",
    "                    ensemble_pred = np.sum(model_preds_weighted, axis=0)\n",
    "\n",
    "                all_preds.append(ensemble_pred)\n",
    "        return np.concatenate(all_preds)\n",
    "\n",
    "ensemble = EnsembleModel(models_to_train, model_states, val_losses, device)\n",
    "print(f\"Ensemble created with {len(ensemble.models)} models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689ff763",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_df.groupby('image_path').first().reset_index()\n",
    "test_images['image_path'] = test_images['image_path'].apply(lambda x: os.path.join(DATA_DIR, x))\n",
    "\n",
    "test_dataset = BiomassDataset(test_images, transform=transform_val)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "all_test_preds = ensemble.predict(test_loader, use_tta=True)\n",
    "\n",
    "submission = []\n",
    "for _, row in test_df.iterrows():\n",
    "    image_path = row['image_path']\n",
    "    target_name = row['target_name']\n",
    "\n",
    "    full_img_path = os.path.join(DATA_DIR, image_path)\n",
    "    img_idx = test_images[test_images['image_path'] == full_img_path].index[0]\n",
    "    target_idx = target_names.index(target_name)\n",
    "    prediction = all_test_preds[img_idx, target_idx]\n",
    "\n",
    "    submission.append({'sample_id': row['sample_id'], 'target': max(0, float(prediction))})\n",
    "\n",
    "submission_df = pd.DataFrame(submission)\n",
    "submission_df.to_csv(f'{WORKING_DIR}/submission.csv', index=False)\n",
    "print('Submission saved to:', f'{WORKING_DIR}/submission.csv')\n",
    "print(submission_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
