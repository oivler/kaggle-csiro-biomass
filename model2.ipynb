{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = '/kaggle/input/csiro-biomass'\n",
    "WORKING_DIR = '/kaggle/working'\n",
    "SIGLIP_PATH = '/kaggle/input/siglip/keras/siglip_so400m_patch14_384/1'\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "TARGET_COLS = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "\n",
    "print('imports done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "pivot = (\n",
    "    train_df.pivot_table(index='image_path', columns='target_name', values='target', aggfunc='first')\n",
    "    .reset_index()\n",
    ")\n",
    "for t in TARGET_COLS:\n",
    "    if t not in pivot.columns:\n",
    "        pivot[t] = np.nan\n",
    "pivot = pivot[['image_path'] + TARGET_COLS]\n",
    "\n",
    "meta_cols = [c for c in train_df.columns if c not in ('sample_id', 'target_name', 'target')]\n",
    "meta_first = train_df.groupby('image_path', as_index=False)[meta_cols].first()\n",
    "train_data = meta_first.merge(pivot, on='image_path', how='left')\n",
    "train_data['full_image_path'] = train_data['image_path'].apply(lambda p: os.path.join(DATA_DIR, p))\n",
    "\n",
    "test_images = test_df.groupby('image_path').first().reset_index()\n",
    "test_images['full_image_path'] = test_images['image_path'].apply(lambda p: os.path.join(DATA_DIR, p))\n",
    "\n",
    "print('train_data', train_data.shape)\n",
    "print('test_images', test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_hub\n",
    "import keras\n",
    "\n",
    "print('keras version:', keras.__version__)\n",
    "print('keras_hub version:', keras_hub.__version__)\n",
    "\n",
    "print('\\nLoading SigLIP model...')\n",
    "siglip_model = keras.saving.load_model(SIGLIP_PATH, compile=False)\n",
    "print('SigLIP loaded successfully')\n",
    "print('Model type:', type(siglip_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 384\n",
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "IMAGENET_STD = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return np.zeros((1, IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = (img - IMAGENET_MEAN) / IMAGENET_STD\n",
    "    return np.expand_dims(img, axis=0).astype(np.float32)\n",
    "\n",
    "def extract_embedding(image_path, model):\n",
    "    try:\n",
    "        img_batch = preprocess_image(image_path)\n",
    "        \n",
    "        if hasattr(model, 'vision_encoder'):\n",
    "            output = model.vision_encoder(img_batch, training=False)\n",
    "        else:\n",
    "            output = model(img_batch, training=False)\n",
    "        \n",
    "        if isinstance(output, dict):\n",
    "            if 'image_embedding' in output:\n",
    "                emb = output['image_embedding']\n",
    "            elif 'pooled_output' in output:\n",
    "                emb = output['pooled_output']\n",
    "            else:\n",
    "                emb = list(output.values())[0]\n",
    "        else:\n",
    "            emb = output\n",
    "        \n",
    "        emb = emb.numpy() if hasattr(emb, 'numpy') else np.array(emb)\n",
    "        \n",
    "        if len(emb.shape) == 4:\n",
    "            emb = np.mean(emb, axis=(1, 2))[0]\n",
    "        elif len(emb.shape) == 3:\n",
    "            emb = emb[0, 0]\n",
    "        elif len(emb.shape) == 2:\n",
    "            emb = emb[0]\n",
    "        else:\n",
    "            emb = emb.flatten()\n",
    "        \n",
    "        return emb.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {image_path}: {e}')\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print('Testing embedding extraction...')\n",
    "test_emb = extract_embedding(train_data['full_image_path'].values[0], siglip_model)\n",
    "if test_emb is not None:\n",
    "    print(f'Embedding shape: {test_emb.shape}')\n",
    "    EMB_DIM = test_emb.shape[0]\n",
    "else:\n",
    "    print('Embedding extraction failed!')\n",
    "    raise RuntimeError('Could not extract embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Extracting train embeddings...')\n",
    "train_embeddings = []\n",
    "for path in tqdm(train_data['full_image_path'].values):\n",
    "    emb = extract_embedding(path, siglip_model)\n",
    "    if emb is None:\n",
    "        emb = np.zeros(EMB_DIM, dtype=np.float32)\n",
    "    train_embeddings.append(emb)\n",
    "train_embeddings = np.stack(train_embeddings)\n",
    "print('train_embeddings', train_embeddings.shape)\n",
    "\n",
    "print('Extracting test embeddings...')\n",
    "test_embeddings = []\n",
    "for path in tqdm(test_images['full_image_path'].values):\n",
    "    emb = extract_embedding(path, siglip_model)\n",
    "    if emb is None:\n",
    "        emb = np.zeros(EMB_DIM, dtype=np.float32)\n",
    "    test_embeddings.append(emb)\n",
    "test_embeddings = np.stack(test_embeddings)\n",
    "print('test_embeddings', test_embeddings.shape)\n",
    "\n",
    "del siglip_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    HAS_CATBOOST = True\n",
    "except:\n",
    "    HAS_CATBOOST = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    HAS_LGBM = True\n",
    "except:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "print(f'CatBoost: {HAS_CATBOOST}, LightGBM: {HAS_LGBM}')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_embeddings)\n",
    "X_test_scaled = scaler.transform(test_embeddings)\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=SEED)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(f'PCA components: {X_train_pca.shape[1]}')\n",
    "\n",
    "y_train = train_data[TARGET_COLS].values.astype(np.float32)\n",
    "print('y_train', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "def train_predict_target(X_train, y_train, X_test, target_idx, target_name):\n",
    "    fold_preds = []\n",
    "    \n",
    "    for fold, (tr_idx, va_idx) in enumerate(kfold.split(X_train)):\n",
    "        X_tr, X_va = X_train[tr_idx], X_train[va_idx]\n",
    "        y_tr, y_va = y_train[tr_idx, target_idx], y_train[va_idx, target_idx]\n",
    "        \n",
    "        model_preds = []\n",
    "        \n",
    "        hist = HistGradientBoostingRegressor(\n",
    "            max_iter=500, learning_rate=0.05, max_depth=6,\n",
    "            l2_regularization=0.5, random_state=SEED + fold\n",
    "        )\n",
    "        hist.fit(X_tr, y_tr)\n",
    "        model_preds.append(hist.predict(X_test))\n",
    "        \n",
    "        if HAS_CATBOOST:\n",
    "            cat = CatBoostRegressor(\n",
    "                iterations=800, learning_rate=0.05, depth=6,\n",
    "                l2_leaf_reg=0.5, random_seed=SEED + fold, verbose=0\n",
    "            )\n",
    "            cat.fit(X_tr, y_tr)\n",
    "            model_preds.append(cat.predict(X_test))\n",
    "        \n",
    "        if HAS_LGBM:\n",
    "            lgbm = LGBMRegressor(\n",
    "                n_estimators=800, learning_rate=0.05, max_depth=6,\n",
    "                reg_lambda=0.5, random_state=SEED + fold, verbose=-1\n",
    "            )\n",
    "            lgbm.fit(X_tr, y_tr)\n",
    "            model_preds.append(lgbm.predict(X_test))\n",
    "        \n",
    "        fold_pred = np.mean(model_preds, axis=0)\n",
    "        fold_preds.append(fold_pred)\n",
    "    \n",
    "    final_pred = np.mean(fold_preds, axis=0)\n",
    "    return final_pred\n",
    "\n",
    "print('Training models for each target...')\n",
    "all_preds = np.zeros((len(X_test_pca), len(TARGET_COLS)), dtype=np.float32)\n",
    "\n",
    "for i, target_name in enumerate(TARGET_COLS):\n",
    "    print(f'  {target_name}...')\n",
    "    pred = train_predict_target(X_train_pca, y_train, X_test_pca, i, target_name)\n",
    "    all_preds[:, i] = pred\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_predictions(preds):\n",
    "    preds = preds.copy()\n",
    "    preds = np.maximum(preds, 0.0)\n",
    "    \n",
    "    green = preds[:, 2]\n",
    "    dead = preds[:, 1]\n",
    "    \n",
    "    preds[:, 0] = 0.0\n",
    "    preds[:, 4] = green\n",
    "    preds[:, 3] = green + dead\n",
    "    \n",
    "    return preds\n",
    "\n",
    "final_preds = post_process_predictions(all_preds)\n",
    "\n",
    "preds_df = pd.DataFrame(\n",
    "    {\n",
    "        'image_path': test_images['image_path'].values,\n",
    "        **{c: final_preds[:, i] for i, c in enumerate(TARGET_COLS)},\n",
    "    }\n",
    ")\n",
    "\n",
    "merged = test_df.merge(preds_df, on='image_path', how='left')\n",
    "\n",
    "rows = []\n",
    "for _, r in merged.iterrows():\n",
    "    rows.append({'sample_id': r['sample_id'], 'target': max(0.0, float(r[r['target_name']]))})\n",
    "\n",
    "sub = pd.DataFrame(rows)\n",
    "sub.to_csv(f'{WORKING_DIR}/submission.csv', index=False)\n",
    "print(sub.head())\n",
    "print('saved', f'{WORKING_DIR}/submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
