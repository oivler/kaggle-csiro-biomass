{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "DATA_DIR = '/kaggle/input/csiro-biomass'\n",
        "WORKING_DIR = '/kaggle/working'\n",
        "\n",
        "USE_PRETRAINED_WEIGHTS = True\n",
        "PRETRAINED_WEIGHTS_DIR = '/kaggle/input/pretrained-weights/pretrained_weights'\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "print('torch', torch.__version__)\n",
        "print('cuda', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('device', torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
        "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
        "\n",
        "TARGET_COLS = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
        "\n",
        "pivot = (\n",
        "    train_df.pivot_table(index='image_path', columns='target_name', values='target', aggfunc='first')\n",
        "    .reset_index()\n",
        ")\n",
        "for t in TARGET_COLS:\n",
        "    if t not in pivot.columns:\n",
        "        pivot[t] = np.nan\n",
        "pivot = pivot[['image_path'] + TARGET_COLS]\n",
        "\n",
        "meta_cols = [c for c in train_df.columns if c not in ('sample_id', 'target_name', 'target')]\n",
        "meta_first = train_df.groupby('image_path', as_index=False)[meta_cols].first()\n",
        "train_data = meta_first.merge(pivot, on='image_path', how='left')\n",
        "\n",
        "train_data['full_image_path'] = train_data['image_path'].apply(lambda p: os.path.join(DATA_DIR, p))\n",
        "\n",
        "species_codes, species_uniques = pd.factorize(train_data['Species'].astype('string'), sort=True)\n",
        "state_codes, state_uniques = pd.factorize(train_data['State'].astype('string'), sort=True)\n",
        "train_data['species_idx'] = species_codes.astype('int64')\n",
        "train_data['state_idx'] = state_codes.astype('int64')\n",
        "\n",
        "train_data['ndvi'] = train_data['Pre_GSHH_NDVI'].astype('float32')\n",
        "train_data['height'] = train_data['Height_Ave_cm'].astype('float32')\n",
        "\n",
        "train_data = train_data.drop(columns=['Pre_GSHH_NDVI', 'Height_Ave_cm'])\n",
        "\n",
        "print('train_data', train_data.shape)\n",
        "print('n_species', len(species_uniques), 'n_state', len(state_uniques))\n",
        "print(train_data[['image_path'] + TARGET_COLS + ['ndvi','height','species_idx','state_idx']].head())\n",
        "\n",
        "test_images = test_df.groupby('image_path').first().reset_index()\n",
        "test_images['full_image_path'] = test_images['image_path'].apply(lambda p: os.path.join(DATA_DIR, p))\n",
        "print('test_images', test_images.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMG_SIZE = 384\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, df, transform, has_targets: bool):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.has_targets = has_targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row['full_image_path']).convert('RGB')\n",
        "        x = self.transform(img) if self.transform else img\n",
        "\n",
        "        if not self.has_targets:\n",
        "            y = torch.zeros(5, dtype=torch.float32)\n",
        "            aux = {\n",
        "                'ndvi': torch.tensor(0.0, dtype=torch.float32),\n",
        "                'height': torch.tensor(0.0, dtype=torch.float32),\n",
        "                'species': torch.tensor(0, dtype=torch.long),\n",
        "                'state': torch.tensor(0, dtype=torch.long),\n",
        "            }\n",
        "            return x, y, aux\n",
        "\n",
        "        y = torch.tensor([row[c] for c in TARGET_COLS], dtype=torch.float32)\n",
        "        aux = {\n",
        "            'ndvi': torch.tensor(float(row['ndvi']), dtype=torch.float32),\n",
        "            'height': torch.tensor(float(row['height']), dtype=torch.float32),\n",
        "            'species': torch.tensor(int(row['species_idx']), dtype=torch.long),\n",
        "            'state': torch.tensor(int(row['state_idx']), dtype=torch.long),\n",
        "        }\n",
        "        return x, y, aux\n",
        "\n",
        "print('dataset ready')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pretrained_weights(model, weights_path):\n",
        "    if os.path.exists(weights_path):\n",
        "        state_dict = torch.load(weights_path, map_location='cpu')\n",
        "        if isinstance(state_dict, dict) and 'state_dict' in state_dict:\n",
        "            state_dict = state_dict['state_dict']\n",
        "        if isinstance(state_dict, dict) and any(k.startswith('module.') for k in state_dict.keys()):\n",
        "            state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "        filtered = {k: v for k, v in state_dict.items() if not (k.startswith('fc.') or k.startswith('classifier.'))}\n",
        "        model.load_state_dict(filtered, strict=False)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "class MultiTaskEffNetB0(nn.Module):\n",
        "    def __init__(self, n_species: int, n_state: int, pretrained_path: str | None):\n",
        "        super().__init__()\n",
        "        self.backbone = models.efficientnet_b0(weights=None)\n",
        "        if pretrained_path:\n",
        "            load_pretrained_weights(self.backbone, pretrained_path)\n",
        "        feat_dim = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "\n",
        "        self.biomass_head = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(feat_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 5),\n",
        "        )\n",
        "\n",
        "        self.reg_head = nn.Sequential(\n",
        "            nn.Linear(feat_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "        self.species_head = nn.Linear(feat_dim, n_species)\n",
        "        self.state_head = nn.Linear(feat_dim, n_state)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.backbone(x)\n",
        "        y = self.biomass_head(feat)\n",
        "        reg = self.reg_head(feat)\n",
        "        sp = self.species_head(feat)\n",
        "        st = self.state_head(feat)\n",
        "        return y, reg, sp, st\n",
        "\n",
        "class MultiTaskResNet34(nn.Module):\n",
        "    def __init__(self, n_species: int, n_state: int, pretrained_path: str | None):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet34(weights=None)\n",
        "        if pretrained_path:\n",
        "            load_pretrained_weights(self.backbone, pretrained_path)\n",
        "        feat_dim = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.biomass_head = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(feat_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 5),\n",
        "        )\n",
        "\n",
        "        self.reg_head = nn.Sequential(\n",
        "            nn.Linear(feat_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "        self.species_head = nn.Linear(feat_dim, n_species)\n",
        "        self.state_head = nn.Linear(feat_dim, n_state)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.backbone(x)\n",
        "        y = self.biomass_head(feat)\n",
        "        reg = self.reg_head(feat)\n",
        "        sp = self.species_head(feat)\n",
        "        st = self.state_head(feat)\n",
        "        return y, reg, sp, st\n",
        "\n",
        "class MultiTaskResNet50(nn.Module):\n",
        "    def __init__(self, n_species: int, n_state: int, pretrained_path: str | None):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet50(weights=None)\n",
        "        if pretrained_path:\n",
        "            load_pretrained_weights(self.backbone, pretrained_path)\n",
        "        feat_dim = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.biomass_head = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(feat_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 5),\n",
        "        )\n",
        "\n",
        "        self.reg_head = nn.Sequential(\n",
        "            nn.Linear(feat_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "        self.species_head = nn.Linear(feat_dim, n_species)\n",
        "        self.state_head = nn.Linear(feat_dim, n_state)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.backbone(x)\n",
        "        y = self.biomass_head(feat)\n",
        "        reg = self.reg_head(feat)\n",
        "        sp = self.species_head(feat)\n",
        "        st = self.state_head(feat)\n",
        "        return y, reg, sp, st\n",
        "\n",
        "print('models ready')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "batch_size = 12\n",
        "num_workers = 2 if torch.cuda.is_available() else 0\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "weight_paths = {}\n",
        "if USE_PRETRAINED_WEIGHTS:\n",
        "    weight_paths['effnetb0'] = os.path.join(PRETRAINED_WEIGHTS_DIR, 'efficientnet_b0_rwightman-3dd342df.pth')\n",
        "    weight_paths['resnet34'] = os.path.join(PRETRAINED_WEIGHTS_DIR, 'resnet34-b627a593.pth')\n",
        "    weight_paths['resnet50'] = os.path.join(PRETRAINED_WEIGHTS_DIR, 'resnet50-0676ba61.pth')\n",
        "    for k, v in list(weight_paths.items()):\n",
        "        if not os.path.exists(v):\n",
        "            del weight_paths[k]\n",
        "\n",
        "loss_biomass = nn.HuberLoss(delta=1.0)\n",
        "loss_reg = nn.MSELoss()\n",
        "loss_ce = nn.CrossEntropyLoss()\n",
        "loss_mse = nn.MSELoss()\n",
        "\n",
        "ALPHA_REG = 0.2\n",
        "BETA_CLS = 0.1\n",
        "GAMMA_CONSTRAINT = 0.01\n",
        "MIXUP_ALPHA = 0.2\n",
        "USE_MIXUP = False\n",
        "\n",
        "model_states = {}\n",
        "val_losses = {}\n",
        "\n",
        "backbone_configs = [\n",
        "    ('effnetb0', MultiTaskEffNetB0, weight_paths.get('effnetb0')),\n",
        "    ('resnet34', MultiTaskResNet34, weight_paths.get('resnet34')),\n",
        "    ('resnet50', MultiTaskResNet50, weight_paths.get('resnet50')),\n",
        "]\n",
        "\n",
        "for backbone_name, ModelClass, weight_path in backbone_configs:\n",
        "    if weight_path is None and USE_PRETRAINED_WEIGHTS:\n",
        "        print(f'Skipping {backbone_name} - no pretrained weights')\n",
        "        continue\n",
        "\n",
        "    for fold_idx, (tr_idx, va_idx) in enumerate(kfold.split(train_data)):\n",
        "        tr = train_data.iloc[tr_idx].reset_index(drop=True)\n",
        "        va = train_data.iloc[va_idx].reset_index(drop=True)\n",
        "\n",
        "        train_ds = MultiTaskDataset(tr, transform_train, has_targets=True)\n",
        "        val_ds = MultiTaskDataset(va, transform_val, has_targets=True)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "        model = ModelClass(len(species_uniques), len(state_uniques), weight_path).to(device)\n",
        "        opt = optim.AdamW(model.parameters(), lr=8e-4, weight_decay=1e-4)\n",
        "        \n",
        "        warmup_epochs = 3\n",
        "        total_epochs = 50\n",
        "        warmup_sched = optim.lr_scheduler.LinearLR(opt, start_factor=0.1, total_iters=warmup_epochs)\n",
        "        cosine_sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=total_epochs - warmup_epochs, eta_min=1e-6)\n",
        "\n",
        "        best = 1e18\n",
        "        best_state = None\n",
        "        second_best = 1e18\n",
        "        second_best_state = None\n",
        "        patience = 10\n",
        "        no_improve = 0\n",
        "\n",
        "        for epoch in range(total_epochs):\n",
        "            model.train()\n",
        "            for x, y, aux in train_loader:\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                ndvi = aux['ndvi'].to(device)\n",
        "                height = aux['height'].to(device)\n",
        "                sp = aux['species'].to(device)\n",
        "                st = aux['state'].to(device)\n",
        "\n",
        "                if USE_MIXUP and np.random.random() < 0.5:\n",
        "                    x, y_a, y_b, lam = mixup_data(x, y, MIXUP_ALPHA)\n",
        "                    opt.zero_grad()\n",
        "                    y_hat, reg_hat, sp_hat, st_hat = model(x)\n",
        "                    l_main = lam * loss_biomass(y_hat, y_a) + (1 - lam) * loss_biomass(y_hat, y_b)\n",
        "                    l_reg = loss_reg(reg_hat[:, 0], ndvi) + loss_reg(reg_hat[:, 1], height)\n",
        "                    l_cls = loss_ce(sp_hat, sp) + loss_ce(st_hat, st)\n",
        "                    l_constraint = loss_mse(y_hat[:, 4], y_hat[:, 0] + y_hat[:, 2]) + loss_mse(y_hat[:, 3], y_hat[:, 4] + y_hat[:, 1])\n",
        "                else:\n",
        "                    opt.zero_grad()\n",
        "                    y_hat, reg_hat, sp_hat, st_hat = model(x)\n",
        "                    l_main = loss_biomass(y_hat, y)\n",
        "                    l_reg = loss_reg(reg_hat[:, 0], ndvi) + loss_reg(reg_hat[:, 1], height)\n",
        "                    l_cls = loss_ce(sp_hat, sp) + loss_ce(st_hat, st)\n",
        "                    l_constraint = loss_mse(y_hat[:, 4], y_hat[:, 0] + y_hat[:, 2]) + loss_mse(y_hat[:, 3], y_hat[:, 4] + y_hat[:, 1])\n",
        "\n",
        "                loss = l_main + ALPHA_REG * l_reg + BETA_CLS * l_cls + GAMMA_CONSTRAINT * l_constraint\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                opt.step()\n",
        "\n",
        "            model.eval()\n",
        "            val_main = 0.0\n",
        "            with torch.no_grad():\n",
        "                for x, y, aux in val_loader:\n",
        "                    x = x.to(device)\n",
        "                    y = y.to(device)\n",
        "                    y_hat, _, _, _ = model(x)\n",
        "                    val_main += loss_biomass(y_hat, y).item()\n",
        "            val_main /= max(1, len(val_loader))\n",
        "            \n",
        "            if epoch < warmup_epochs:\n",
        "                warmup_sched.step()\n",
        "            else:\n",
        "                cosine_sched.step()\n",
        "\n",
        "            if val_main < best:\n",
        "                second_best = best\n",
        "                second_best_state = best_state\n",
        "                best = val_main\n",
        "                best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
        "                no_improve = 0\n",
        "            elif val_main < second_best:\n",
        "                second_best = val_main\n",
        "                second_best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
        "                no_improve += 1\n",
        "            else:\n",
        "                no_improve += 1\n",
        "\n",
        "            print(f'{backbone_name} fold {fold_idx+1} epoch {epoch+1}/{total_epochs} val_main {val_main:.4f} lr {opt.param_groups[0][\"lr\"]:.6f}')\n",
        "            \n",
        "            if no_improve >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "        model_states[f'{backbone_name}_fold{fold_idx+1}'] = best_state\n",
        "        val_losses[f'{backbone_name}_fold{fold_idx+1}'] = best\n",
        "        \n",
        "        if second_best_state is not None:\n",
        "            model_states[f'{backbone_name}_fold{fold_idx+1}_2nd'] = second_best_state\n",
        "            val_losses[f'{backbone_name}_fold{fold_idx+1}_2nd'] = second_best\n",
        "\n",
        "print('done', len(model_states))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def post_process_predictions(preds):\n",
        "    preds = preds.copy()\n",
        "    preds = np.maximum(preds, 0.0)\n",
        "    \n",
        "    clover = preds[:, 0]\n",
        "    dead = preds[:, 1]\n",
        "    green = preds[:, 2]\n",
        "    total = preds[:, 3]\n",
        "    gdm = preds[:, 4]\n",
        "    \n",
        "    gdm_recalc = green + clover\n",
        "    gdm = np.maximum(gdm, gdm_recalc * 0.95)\n",
        "    \n",
        "    total_recalc = gdm + dead\n",
        "    total = np.maximum(total, total_recalc * 0.95)\n",
        "    \n",
        "    preds[:, 0] = clover\n",
        "    preds[:, 1] = dead\n",
        "    preds[:, 2] = green\n",
        "    preds[:, 3] = total\n",
        "    preds[:, 4] = gdm\n",
        "    \n",
        "    return preds\n",
        "\n",
        "class BiomassEnsemble:\n",
        "    def __init__(self, model_states: dict, val_losses: dict):\n",
        "        inv = {k: 1.0 / (v + 1e-8) for k, v in val_losses.items()}\n",
        "        s = sum(inv.values())\n",
        "        self.weights = {k: inv[k] / s for k in inv}\n",
        "        self.models = {}\n",
        "        \n",
        "        for name, state in model_states.items():\n",
        "            backbone_name = name.split('_fold')[0]\n",
        "            weight_path = weight_paths.get(backbone_name)\n",
        "            \n",
        "            if backbone_name == 'effnetb0':\n",
        "                m = MultiTaskEffNetB0(len(species_uniques), len(state_uniques), weight_path).to(device)\n",
        "            elif backbone_name == 'resnet34':\n",
        "                m = MultiTaskResNet34(len(species_uniques), len(state_uniques), weight_path).to(device)\n",
        "            elif backbone_name == 'resnet50':\n",
        "                m = MultiTaskResNet50(len(species_uniques), len(state_uniques), weight_path).to(device)\n",
        "            else:\n",
        "                continue\n",
        "            \n",
        "            m.load_state_dict(state)\n",
        "            m.eval()\n",
        "            self.models[name] = m\n",
        "\n",
        "    def predict(self, loader, use_tta=True):\n",
        "        all_preds = []\n",
        "        with torch.no_grad():\n",
        "            for x, _, _ in loader:\n",
        "                x = x.to(device)\n",
        "                batch_preds = []\n",
        "                \n",
        "                if use_tta:\n",
        "                    tta_transforms = [\n",
        "                        lambda img: img,\n",
        "                        lambda img: torch.flip(img, [3]),\n",
        "                        lambda img: torch.flip(img, [2]),\n",
        "                    ]\n",
        "                    \n",
        "                    for tta_fn in tta_transforms:\n",
        "                        x_aug = tta_fn(x)\n",
        "                        out = None\n",
        "                        for name, m in self.models.items():\n",
        "                            y_hat, _, _, _ = m(x_aug)\n",
        "                            w = self.weights.get(name, 1.0 / len(self.models))\n",
        "                            out = y_hat * w if out is None else out + y_hat * w\n",
        "                        batch_preds.append(out.cpu().numpy())\n",
        "                    \n",
        "                    ensemble_pred = np.mean(batch_preds, axis=0)\n",
        "                else:\n",
        "                    out = None\n",
        "                    for name, m in self.models.items():\n",
        "                        y_hat, _, _, _ = m(x)\n",
        "                        w = self.weights.get(name, 1.0 / len(self.models))\n",
        "                        out = y_hat * w if out is None else out + y_hat * w\n",
        "                    ensemble_pred = out.cpu().numpy()\n",
        "                \n",
        "                all_preds.append(ensemble_pred)\n",
        "        \n",
        "        preds = np.concatenate(all_preds, axis=0)\n",
        "        return post_process_predictions(preds)\n",
        "\n",
        "\n",
        "test_ds = MultiTaskDataset(test_images, transform_val, has_targets=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "ens = BiomassEnsemble(model_states, val_losses)\n",
        "all_test_preds_cnn = ens.predict(test_loader, use_tta=True)\n",
        "\n",
        "preds_df_cnn = pd.DataFrame(\n",
        "    {\n",
        "        'image_path': test_images['image_path'].values,\n",
        "        **{c: all_test_preds_cnn[:, i] for i, c in enumerate(TARGET_COLS)},\n",
        "    }\n",
        ")\n",
        "\n",
        "print('CNN predictions done, shape:', all_test_preds_cnn.shape)\n",
        "print(f'Ensemble has {len(ens.models)} models')\n",
        "\n",
        "USE_TABULAR_BLEND = False\n",
        "CNN_WEIGHT = 1.0\n",
        "TAB_WEIGHT = 0.0\n",
        "\n",
        "if USE_TABULAR_BLEND:\n",
        "    print('Attempting tabular blend...')\n",
        "    try:\n",
        "        tabular_file = f'{WORKING_DIR}/tabular_predictions.csv'\n",
        "        if os.path.exists(tabular_file):\n",
        "            tabular_preds_df = pd.read_csv(tabular_file)\n",
        "            blended = preds_df_cnn.merge(tabular_preds_df, on='image_path', how='left', suffixes=('_cnn', '_tab'))\n",
        "            \n",
        "            CNN_WEIGHT = 0.90\n",
        "            TAB_WEIGHT = 0.10\n",
        "            \n",
        "            for t in TARGET_COLS:\n",
        "                cnn_col = f'{t}_cnn'\n",
        "                tab_col = f'{t}_tab'\n",
        "                if tab_col in blended.columns:\n",
        "                    blended[t] = CNN_WEIGHT * blended[cnn_col].fillna(0) + TAB_WEIGHT * blended[tab_col].fillna(0)\n",
        "                else:\n",
        "                    blended[t] = blended[cnn_col]\n",
        "            \n",
        "            preds_df = blended[['image_path'] + TARGET_COLS]\n",
        "            print(f'Blended CNN ({CNN_WEIGHT*100:.0f}%) + Tabular ({TAB_WEIGHT*100:.0f}%)')\n",
        "        else:\n",
        "            print('Tabular predictions not found, using CNN only')\n",
        "            preds_df = preds_df_cnn\n",
        "    except Exception as e:\n",
        "        print(f'Tabular blending failed: {e}, using CNN only')\n",
        "        preds_df = preds_df_cnn\n",
        "else:\n",
        "    preds_df = preds_df_cnn\n",
        "\n",
        "merged = test_df.merge(preds_df, on='image_path', how='left')\n",
        "\n",
        "rows = []\n",
        "for _, r in merged.iterrows():\n",
        "    rows.append({'sample_id': r['sample_id'], 'target': max(0.0, float(r[r['target_name']]))})\n",
        "\n",
        "sub = pd.DataFrame(rows)\n",
        "sub.to_csv(f'{WORKING_DIR}/submission.csv', index=False)\n",
        "print(sub.head())\n",
        "print('saved', f'{WORKING_DIR}/submission.csv')"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
