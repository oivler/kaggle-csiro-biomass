{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "from google.protobuf import message_factory\n",
    "if not hasattr(message_factory.MessageFactory, 'GetPrototype'):\n",
    "    def _GetPrototype(self, descriptor):\n",
    "        return self.GetMessageClass(descriptor)\n",
    "    message_factory.MessageFactory.GetPrototype = _GetPrototype\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoImageProcessor, AutoTokenizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "DATA_DIR = '/kaggle/input/csiro-biomass'\n",
    "WORKING_DIR = '/kaggle/working'\n",
    "SIGLIP_PATH = '/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1'\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "TARGET_NAMES = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "TARGET_MAX = {\n",
    "    'Dry_Clover_g': 71.7865,\n",
    "    'Dry_Dead_g': 83.8407,\n",
    "    'Dry_Green_g': 157.9836,\n",
    "    'Dry_Total_g': 185.70,\n",
    "    'GDM_g': 157.9836,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_table(df):\n",
    "    if 'target' in df.columns:\n",
    "        df_pt = pd.pivot_table(\n",
    "            df, values='target',\n",
    "            index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "            columns='target_name', aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        df['target'] = 0\n",
    "        df_pt = pd.pivot_table(df, values='target', index='image_path', columns='target_name', aggfunc='mean').reset_index()\n",
    "    return df_pt\n",
    "\n",
    "train_long = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_long = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "train_df = pivot_table(train_long)\n",
    "test_df = pivot_table(test_long)\n",
    "\n",
    "train_df['image_path'] = train_df['image_path'].apply(lambda p: os.path.join(DATA_DIR, p))\n",
    "test_df['image_path'] = test_df['image_path'].apply(lambda p: os.path.join(DATA_DIR, p))\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "train_df['fold'] = -1\n",
    "for fold, (_, val_idx) in enumerate(sgkf.split(train_df, train_df['State'], groups=train_df['Sampling_Date'])):\n",
    "    train_df.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "print('train_df', train_df.shape)\n",
    "print('test_df', test_df.shape)\n",
    "print('Folds:', train_df['fold'].value_counts().sort_index().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading SigLIP')\n",
    "model = AutoModel.from_pretrained(SIGLIP_PATH, local_files_only=True).eval().to(device)\n",
    "processor = AutoImageProcessor.from_pretrained(SIGLIP_PATH)\n",
    "print('SigLIP loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(image, patch_size=520, overlap=16):\n",
    "    h, w, c = image.shape\n",
    "    stride = patch_size - overlap\n",
    "    patches, coords = [], []\n",
    "    for y in range(0, h, stride):\n",
    "        for x in range(0, w, stride):\n",
    "            y1, x1, y2, x2 = y, x, y + patch_size, x + patch_size\n",
    "            patch = image[y1:y2, x1:x2, :]\n",
    "            if patch.shape[0] < patch_size or patch.shape[1] < patch_size:\n",
    "                pad_h = patch_size - patch.shape[0]\n",
    "                pad_w = patch_size - patch.shape[1]\n",
    "                patch = np.pad(patch, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "            patches.append(patch)\n",
    "            coords.append((y1, x1, y2, x2))\n",
    "    return patches, coords\n",
    "\n",
    "def compute_embeddings(df, patch_size=520):\n",
    "    IMAGE_PATHS, EMBEDDINGS = [], []\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        img_path = row['image_path']\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            img = np.zeros((1000, 2000, 3), dtype=np.uint8)\n",
    "        patches, coords = split_image(img, patch_size=patch_size)\n",
    "        images = [Image.fromarray(p).convert('RGB') for p in patches]\n",
    "        inputs = processor(images=images, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            features = model.get_image_features(**inputs)\n",
    "        embeds = features.mean(dim=0).detach().cpu().numpy()\n",
    "        EMBEDDINGS.append(embeds)\n",
    "        IMAGE_PATHS.append(img_path)\n",
    "    embeddings = np.stack(EMBEDDINGS, axis=0)\n",
    "    n_features = embeddings.shape[1]\n",
    "    emb_columns = [f'emb{i+1}' for i in range(n_features)]\n",
    "    emb_df = pd.DataFrame(embeddings, columns=emb_columns)\n",
    "    emb_df['image_path'] = IMAGE_PATHS\n",
    "    df_final = df.merge(emb_df, on='image_path', how='left')\n",
    "    return df_final\n",
    "\n",
    "print('Computing train embeddings...')\n",
    "train_df = compute_embeddings(train_df, patch_size=520)\n",
    "print('Computing test embeddings...')\n",
    "test_df = compute_embeddings(test_df, patch_size=520)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating semantic features...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(SIGLIP_PATH)\n",
    "\n",
    "concept_groups = {\n",
    "    'bare': ['bare soil', 'dirt ground', 'sparse vegetation', 'exposed earth'],\n",
    "    'sparse': ['low density pasture', 'thin grass', 'short clipped grass'],\n",
    "    'medium': ['average pasture cover', 'medium height grass', 'grazed pasture'],\n",
    "    'dense': ['dense tall pasture', 'thick grassy volume', 'high biomass', 'overgrown vegetation'],\n",
    "    'green': ['lush green vibrant pasture', 'photosynthesizing leaves', 'fresh growth'],\n",
    "    'dead': ['dry brown dead grass', 'yellow straw', 'senesced material', 'standing hay'],\n",
    "    'clover': ['white clover', 'trifolium repens', 'broadleaf legume', 'clover flowers'],\n",
    "    'grass': ['ryegrass', 'blade-like leaves', 'fescue', 'grassy sward'],\n",
    "    'weeds': ['broadleaf weeds', 'thistles', 'non-pasture vegetation']\n",
    "}\n",
    "\n",
    "concept_vectors = {}\n",
    "with torch.no_grad():\n",
    "    for name, prompts in concept_groups.items():\n",
    "        inputs = tokenizer(prompts, padding='max_length', return_tensors='pt').to(device)\n",
    "        emb = model.get_text_features(**inputs)\n",
    "        emb = emb / emb.norm(p=2, dim=-1, keepdim=True)\n",
    "        concept_vectors[name] = emb.mean(dim=0, keepdim=True)\n",
    "\n",
    "emb_cols = [c for c in train_df.columns if c.startswith('emb')]\n",
    "X_all_emb = np.vstack([train_df[emb_cols].values, test_df[emb_cols].values])\n",
    "img_tensor = torch.tensor(X_all_emb, dtype=torch.float32).to(device)\n",
    "img_tensor = img_tensor / img_tensor.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "scores = {}\n",
    "for name, vec in concept_vectors.items():\n",
    "    scores[name] = torch.matmul(img_tensor, vec.T).cpu().numpy().flatten()\n",
    "\n",
    "df_scores = pd.DataFrame(scores)\n",
    "df_scores['ratio_greenness'] = df_scores['green'] / (df_scores['green'] + df_scores['dead'] + 1e-6)\n",
    "df_scores['ratio_clover'] = df_scores['clover'] / (df_scores['clover'] + df_scores['grass'] + 1e-6)\n",
    "df_scores['ratio_cover'] = (df_scores['dense'] + df_scores['medium']) / (df_scores['bare'] + df_scores['sparse'] + 1e-6)\n",
    "df_scores['max_density'] = df_scores[['bare', 'sparse', 'medium', 'dense']].max(axis=1)\n",
    "\n",
    "sem_all = df_scores.values\n",
    "n_train = len(train_df)\n",
    "sem_train = sem_all[:n_train]\n",
    "sem_test = sem_all[n_train:]\n",
    "print('Semantic features:', sem_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, processor, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print('Cleaned up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from copy import deepcopy\n",
    "\n",
    "class SupervisedEmbeddingEngine:\n",
    "    def __init__(self, n_pca=0.80, n_pls=8, n_gmm=6, random_state=42):\n",
    "        self.n_pca = n_pca\n",
    "        self.n_pls = n_pls\n",
    "        self.n_gmm = n_gmm\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=n_pca, random_state=random_state)\n",
    "        self.pls = PLSRegression(n_components=n_pls, scale=False)\n",
    "        self.gmm = GaussianMixture(n_components=n_gmm, covariance_type='diag', random_state=random_state)\n",
    "        self.pls_fitted_ = False\n",
    "\n",
    "    def fit(self, X, y=None, X_semantic=None):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.pca.fit(X_scaled)\n",
    "        self.gmm.fit(X_scaled)\n",
    "        if y is not None:\n",
    "            y_clean = y.values if hasattr(y, 'values') else y\n",
    "            self.pls.fit(X_scaled, y_clean)\n",
    "            self.pls_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, X_semantic=None):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        features = [self.pca.transform(X_scaled)]\n",
    "        if self.pls_fitted_:\n",
    "            features.append(self.pls.transform(X_scaled))\n",
    "        features.append(self.gmm.predict_proba(X_scaled))\n",
    "        if X_semantic is not None:\n",
    "            sem_norm = (X_semantic - np.mean(X_semantic, axis=0)) / (np.std(X_semantic, axis=0) + 1e-6)\n",
    "            features.append(sem_norm)\n",
    "        return np.hstack(features)\n",
    "\n",
    "print('Engine ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_max_arr = np.array([TARGET_MAX[t] for t in TARGET_NAMES], dtype=float)\n",
    "emb_cols = [c for c in train_df.columns if c.startswith('emb')]\n",
    "\n",
    "def cross_validate(model_class, train_data, test_data, feat_engine, sem_train, sem_test):\n",
    "    n_splits = train_data['fold'].nunique()\n",
    "    y_true = train_data[TARGET_NAMES]\n",
    "    y_pred = pd.DataFrame(0.0, index=train_data.index, columns=TARGET_NAMES)\n",
    "    y_pred_test = np.zeros([len(test_data), len(TARGET_NAMES)], dtype=float)\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "        train_mask = train_data['fold'] != fold\n",
    "        valid_mask = train_data['fold'] == fold\n",
    "        val_idx = train_data[valid_mask].index\n",
    "        \n",
    "        X_train_raw = train_data[train_mask][emb_cols].values\n",
    "        X_valid_raw = train_data[valid_mask][emb_cols].values\n",
    "        X_test_raw = test_data[emb_cols].values\n",
    "        \n",
    "        sem_train_fold = sem_train[train_mask] if sem_train is not None else None\n",
    "        sem_valid_fold = sem_train[valid_mask] if sem_train is not None else None\n",
    "        \n",
    "        y_train = train_data[train_mask][TARGET_NAMES].values\n",
    "        y_train_proc = y_train / target_max_arr\n",
    "        \n",
    "        engine = deepcopy(feat_engine)\n",
    "        engine.fit(X_train_raw, y=y_train_proc, X_semantic=sem_train_fold)\n",
    "        \n",
    "        x_train_eng = engine.transform(X_train_raw, X_semantic=sem_train_fold)\n",
    "        x_valid_eng = engine.transform(X_valid_raw, X_semantic=sem_valid_fold)\n",
    "        x_test_eng = engine.transform(X_test_raw, X_semantic=sem_test)\n",
    "        \n",
    "        fold_valid_pred = np.zeros_like(train_data[valid_mask][TARGET_NAMES].values)\n",
    "        fold_test_pred = np.zeros([len(test_data), len(TARGET_NAMES)])\n",
    "        \n",
    "        for k in range(len(TARGET_NAMES)):\n",
    "            regr = deepcopy(model_class)\n",
    "            regr.fit(x_train_eng, y_train_proc[:, k])\n",
    "            pred_valid_raw = regr.predict(x_valid_eng)\n",
    "            pred_test_raw = regr.predict(x_test_eng)\n",
    "            fold_valid_pred[:, k] = pred_valid_raw * target_max_arr[k]\n",
    "            fold_test_pred[:, k] = pred_test_raw * target_max_arr[k]\n",
    "        \n",
    "        y_pred.loc[val_idx] = fold_valid_pred\n",
    "        y_pred_test += fold_test_pred / n_splits\n",
    "    \n",
    "    return y_pred.values, y_pred_test\n",
    "\n",
    "feat_engine = SupervisedEmbeddingEngine(n_pca=0.80, n_pls=8, n_gmm=6)\n",
    "\n",
    "print('Training GradientBoosting...')\n",
    "oof_gb, pred_gb = cross_validate(GradientBoostingRegressor(), train_df, test_df, feat_engine, sem_train, sem_test)\n",
    "\n",
    "print('Training HistGradientBoosting...')\n",
    "oof_hb, pred_hb = cross_validate(HistGradientBoostingRegressor(), train_df, test_df, feat_engine, sem_train, sem_test)\n",
    "\n",
    "print('Training CatBoost...')\n",
    "oof_cat, pred_cat = cross_validate(CatBoostRegressor(verbose=0), train_df, test_df, feat_engine, sem_train, sem_test)\n",
    "\n",
    "print('Training LightGBM...')\n",
    "oof_lgbm, pred_lgbm = cross_validate(LGBMRegressor(verbose=-1), train_df, test_df, feat_engine, sem_train, sem_test)\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = (pred_gb + pred_hb + pred_cat + pred_lgbm) / 4.0\n",
    "\n",
    "def post_process_biomass(df_preds):\n",
    "    ordered_cols = ['Dry_Green_g', 'Dry_Clover_g', 'Dry_Dead_g', 'GDM_g', 'Dry_Total_g']\n",
    "    Y = df_preds[ordered_cols].values.T\n",
    "    C = np.array([[1, 1, 0, -1, 0], [0, 0, 1, 1, -1]])\n",
    "    C_T = C.T\n",
    "    inv_CCt = np.linalg.inv(C @ C_T)\n",
    "    P = np.eye(5) - C_T @ inv_CCt @ C\n",
    "    Y_reconciled = P @ Y\n",
    "    Y_reconciled = Y_reconciled.T.clip(min=0)\n",
    "    df_out = df_preds.copy()\n",
    "    df_out[ordered_cols] = Y_reconciled\n",
    "    return df_out\n",
    "\n",
    "test_df[TARGET_NAMES] = pred_test\n",
    "test_df = post_process_biomass(test_df)\n",
    "\n",
    "def melt_table(df):\n",
    "    melted = df.melt(\n",
    "        id_vars='image_path',\n",
    "        value_vars=TARGET_NAMES,\n",
    "        var_name='target_name',\n",
    "        value_name='target'\n",
    "    )\n",
    "    melted['sample_id'] = (\n",
    "        melted['image_path']\n",
    "        .str.replace(r'^.*/', '', regex=True)\n",
    "        .str.replace('.jpg', '', regex=False)\n",
    "        + '__' + melted['target_name']\n",
    "    )\n",
    "    return melted[['sample_id', 'target']]\n",
    "\n",
    "sub = melt_table(test_df)\n",
    "sub.to_csv(f'{WORKING_DIR}/submission.csv', index=False)\n",
    "print(sub.head())\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mods_to_del = [k for k in list(sys.modules.keys()) if any(x in k for x in ['torch', 'transformers', 'huggingface', 'tokenizers', 'safetensors', 'google.protobuf', 'protobuf'])]\n",
    "for m in mods_to_del:\n",
    "    try:\n",
    "        del sys.modules[m]\n",
    "    except:\n",
    "        pass\n",
    "gc.collect()\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
